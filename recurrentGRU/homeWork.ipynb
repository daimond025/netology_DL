{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import random\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786fafb0-f18f-4fbc-ba39-9e736ffecc0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T12:27:15.575312Z",
     "start_time": "2024-12-14T12:27:15.381533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 600893\n"
     ]
    }
   ],
   "source": [
    "with open('data/nietzsche.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('length:', len(text))\n",
    "text = re.sub('[^a-z ]', ' ', text)\n",
    "text = re.sub(r'\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49917ce5-ea3b-49d3-8b10-88d2087b2a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preface supposing that truth is a woman what then is there not ground for suspecting that all philos'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704cbf7-fef0-462c-ae3a-f2cf95a035cc",
   "metadata": {},
   "source": [
    "Составим алфавит кодирофки символов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "152e322d-f585-4637-84d7-ec5d43d3230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TO_CHAR = sorted(list(set(text)))\n",
    "CHAR_TO_INDEX = {c: i for i, c in enumerate(INDEX_TO_CHAR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c20d610-2d3c-4204-b925-9c3a2ba0309c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da5f3e8e-1f46-4c74-8159-5141e3f986db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sents: 193075\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 40\n",
    "STEP = 3\n",
    "SENTENCES = []\n",
    "NEXT_CHARS = []\n",
    "for i in range(0, len(text) - MAX_LEN, STEP):\n",
    "    SENTENCES.append(text[i: i + MAX_LEN])\n",
    "    NEXT_CHARS.append(text[i + MAX_LEN])\n",
    "print('Num sents:', len(SENTENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82fb514d-b308-4af8-bcb9-353842d2a5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = torch.zeros((len(SENTENCES), MAX_LEN), dtype=int)\n",
    "Y = torch.zeros((len(SENTENCES)), dtype=int)\n",
    "for i, sentence in enumerate(SENTENCES):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t] = CHAR_TO_INDEX[char]\n",
    "    Y[i] = CHAR_TO_INDEX[NEXT_CHARS[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56186c32-5e42-4ddd-a510-c7f67d8cca9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16, 18,  5,  6,  1,  3,  5,  0, 19, 21, 16, 16, 15, 19,  9, 14,  7,  0,\n",
       "          20,  8,  1, 20,  0, 20, 18, 21, 20,  8,  0,  9, 19,  0,  1,  0, 23, 15,\n",
       "          13,  1, 14,  0]]),\n",
       " tensor(23))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:1], Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74c0e736-6492-45c4-9291-0efbd0bc4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 256\n",
    "dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "data = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78369770-3e31-44f4-99f0-5905d46fc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(dictionary_size, embedding_size)\n",
    "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True)\n",
    "        self.output = nn.Linear(num_hiddens, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        _, state = self.hidden(out)\n",
    "        predictions = self.output(state[0].squeeze())\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d109e8cf-3f86-4e78-97f5-2168ea878896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(nn.LSTM, len(CHAR_TO_INDEX), 64, 128, len(CHAR_TO_INDEX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d788ddd4-1f43-4b76-aa38-2a3ae81572ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ur through the many finer and coarser mo|wrxbbefntpyphzojgvvol ufizvizzkdosbvxbjt\n"
     ]
    }
   ],
   "source": [
    "def sample(preds):\n",
    "    softmaxed = torch.softmax(preds, 0)\n",
    "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()\n",
    "    return probas.argmax()\n",
    "\n",
    "def generate_text():\n",
    "    start_index = random.randint(0, len(text) - MAX_LEN - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + MAX_LEN]\n",
    "    generated += sentence\n",
    "\n",
    "\n",
    "    for i in range(MAX_LEN):\n",
    "        x_pred = torch.zeros((1, MAX_LEN), dtype=int)\n",
    "        for t, char in enumerate(generated[-MAX_LEN:]):\n",
    "            x_pred[0, t] = CHAR_TO_INDEX[char]\n",
    "\n",
    "        preds = model(x_pred).cpu()\n",
    "        next_char = INDEX_TO_CHAR[sample(preds)]\n",
    "        generated = generated + next_char\n",
    "    print(generated[:MAX_LEN] + '|' + generated[MAX_LEN:])\n",
    "\n",
    "generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4d279de-e39a-4c0b-be23-83f87e2f200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(nn.LSTM, len(CHAR_TO_INDEX), 64, 128, len(CHAR_TO_INDEX))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20b4cab6-6505-483b-9530-2e6e0ca4fe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (embedding): Embedding(27, 64)\n",
       "  (hidden): LSTM(64, 128, batch_first=True)\n",
       "  (output): Linear(in_features=128, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47672004-e800-4bb0-b5a0-8ecfd8181627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 12.462, Train loss: 1.194\n",
      "ss of its advocates than by anything els|e and in the same thes are comes s maste\n",
      "Epoch 1. Time: 11.521, Train loss: 1.186\n",
      "nd peoples must not be estimated by our |faith and judgmes seals understand speci\n",
      "Epoch 2. Time: 11.435, Train loss: 1.177\n",
      "d and evil spirits wage war with varying| permacters he is a hone of lacking it a\n",
      "Epoch 3. Time: 12.464, Train loss: 1.171\n",
      "t this contest should always be kept up |so germans however among to be forgages \n",
      "Epoch 4. Time: 11.487, Train loss: 1.165\n",
      " who cut ruthlessly into his own flesh a|nd because since super who dour at als t\n",
      "Epoch 5. Time: 11.463, Train loss: 1.157\n",
      "they will not only have a smile but a ge|neral equalitues motelour stand is a rul\n",
      "Epoch 6. Time: 11.369, Train loss: 1.151\n",
      "en a sheep for a hero is it so extraordi|nary compusies houses to higher more fre\n",
      "Epoch 7. Time: 11.461, Train loss: 1.145\n",
      "urally suffer in all their scientific ti|mes have though stand within they have n\n",
      "Epoch 8. Time: 11.323, Train loss: 1.139\n",
      "the last judgment upon eternal life here|ly sequired of the stumbly absolated and\n",
      "Epoch 9. Time: 11.323, Train loss: 1.134\n",
      "onsiderate cruelty which knows how to ha|ve teempter as into the opposing hand so\n",
      "Epoch 10. Time: 12.088, Train loss: 1.128\n",
      "is habits two men with the same principl|e slanct of the indifferent the learnt c\n",
      "Epoch 11. Time: 12.405, Train loss: 1.122\n",
      "every change a something isolated discon|tends of the sourbilation and existently\n",
      "Epoch 12. Time: 11.949, Train loss: 1.117\n",
      " a spectacle fit for gods and godlike ma|nkind in cane of anciety of objection an\n",
      "Epoch 13. Time: 12.028, Train loss: 1.112\n",
      "omplete men which at every point also im|pulses in the seeps about the stone as i\n",
      "Epoch 14. Time: 11.924, Train loss: 1.108\n",
      "t in my opinion man is an agreeable brav|est human suspected to grom niferent fro\n",
      "Epoch 15. Time: 12.147, Train loss: 1.103\n",
      "ad propensities insofar as such have the| belief it is there will did methoved of\n",
      "Epoch 16. Time: 12.009, Train loss: 1.099\n",
      "constitutions for example are of this or|ce almost is strive the own varioleds fe\n",
      "Epoch 17. Time: 11.961, Train loss: 1.093\n",
      "ually sufficed for the basis of such imp|athed know to take the master some admir\n",
      "Epoch 18. Time: 12.038, Train loss: 1.089\n",
      "t intentional and that all its intention| of another and at nevertheless that is \n",
      "Epoch 19. Time: 11.921, Train loss: 1.085\n",
      "their tempers they are by no means enemi|ately ears hope of imbery first places t\n",
      "Epoch 20. Time: 12.089, Train loss: 1.081\n",
      "r blame for it is irrational to blame an|d can be regard certainty for own chilon\n",
      "Epoch 21. Time: 12.002, Train loss: 1.076\n",
      " sentinel at the portal of the temple of| into ingrint eyes is infentive in man w\n",
      "Epoch 22. Time: 11.997, Train loss: 1.073\n",
      "ehood why do men as a rule speak the tru|e places as me of a thing over sympathy \n",
      "Epoch 23. Time: 11.948, Train loss: 1.068\n",
      "from this point of view there is perhaps| justice to their really simpliaring and\n",
      "Epoch 24. Time: 12.038, Train loss: 1.065\n",
      "rding to one s capacity for great self s|cience muit the final parention which sp\n",
      "Epoch 25. Time: 12.129, Train loss: 1.062\n",
      "ething else for a pleasure namely the un|contradisses are regarded and the object\n",
      "Epoch 26. Time: 12.137, Train loss: 1.058\n",
      "type of new germanism is covetous of qui| obedined philosopher autonsal distictib\n",
      "Epoch 27. Time: 11.953, Train loss: 1.055\n",
      "istent dreadful will of its own that can| better will problek them breadied affix\n",
      "Epoch 28. Time: 12.076, Train loss: 1.050\n",
      "lence of concealment he who has sat day |sensation of the abstrod of posing fach \n",
      "Epoch 29. Time: 11.969, Train loss: 1.047\n",
      "is like everything habitual and natural |severeiming against give in itself again\n",
      "Epoch 30. Time: 11.992, Train loss: 1.043\n",
      " the discovery has been made that in usi|ble intersponsibil the are us the myself\n",
      "Epoch 31. Time: 11.945, Train loss: 1.040\n",
      "e must necessarily be something that thi|s fanalues to a rouble against the finer\n",
      "Epoch 32. Time: 12.105, Train loss: 1.038\n",
      " whole profession and as i have said his| demand the warted venebation from him h\n",
      "Epoch 33. Time: 11.753, Train loss: 1.035\n",
      "present the great discharge from all the| goes almost their looked internal carti\n",
      "Epoch 34. Time: 11.415, Train loss: 1.030\n",
      "he comprehension of men that they neithe|r states pantient vicourable but of ever\n",
      "Epoch 35. Time: 11.341, Train loss: 1.028\n",
      " polishing soul no longer knows how to a|ssicide instinct of the piry merely inst\n",
      "Epoch 36. Time: 12.151, Train loss: 1.025\n",
      "e kind of small independent clock work w|ithin close to the greater in order the \n",
      "Epoch 37. Time: 12.548, Train loss: 1.023\n",
      "eaks without bitterness or rather quite |this last and he he flood this believes \n",
      "Epoch 38. Time: 12.232, Train loss: 1.019\n",
      "ist the ideal man of learning in whom th|e wantorishans of his cature as if non a\n",
      "Epoch 39. Time: 11.453, Train loss: 1.016\n",
      "flesh as into the flesh and heart of the| race brance with a science and schopons\n",
      "Epoch 40. Time: 11.409, Train loss: 1.014\n",
      "ls his spirit to perceive against its ow|it the practificed and jistoxited former\n",
      "Epoch 41. Time: 11.331, Train loss: 1.011\n",
      "the good conscience of his tolerance in |shens of manking of every tention even e\n",
      "Epoch 42. Time: 11.725, Train loss: 1.009\n",
      " antique taste by the paradox of the for|mer too it is too nature badl would has \n",
      "Epoch 43. Time: 11.341, Train loss: 1.007\n",
      "ustomed to lying or to express it more p|lace and also ir any cates supposing and\n",
      "Epoch 44. Time: 11.909, Train loss: 1.004\n",
      "ey are over spiced and begin to smell da|y in a pestim as thor that no conscience\n",
      "Epoch 45. Time: 12.577, Train loss: 1.002\n",
      "lmost in love with it until they become |stowns sufficient development of demands\n",
      "Epoch 46. Time: 12.763, Train loss: 0.998\n",
      "ain is never the same as the sensation t|his which the rational given conceptions\n",
      "Epoch 47. Time: 12.638, Train loss: 0.998\n",
      "ty and hardness and has reverence for al|trowhthy freed in the tant is no contrac\n",
      "Epoch 48. Time: 12.760, Train loss: 0.996\n",
      "in itself therefore it is evil a belief |has hitherto been secure amoggatic and r\n",
      "Epoch 49. Time: 11.929, Train loss: 0.992\n",
      "ravagant boundlessly indifferent without| only deception the relations with this \n",
      "Epoch 50. Time: 11.712, Train loss: 0.991\n",
      "ly tend in the most marked way to develo|p with the worst to gressnous estrumente\n",
      "Epoch 51. Time: 11.275, Train loss: 0.988\n",
      "ure to avow a new desire a dissatisfacti|on in god there are in it out regarded o\n",
      "Epoch 52. Time: 11.273, Train loss: 0.986\n",
      " which serves the higher class of men fo|r a mind and a barbarity and who will th\n",
      "Epoch 53. Time: 11.847, Train loss: 0.984\n",
      " mind the former conception which had to| sin one brangest weality when they is h\n",
      "Epoch 54. Time: 11.845, Train loss: 0.982\n",
      " wisdom worldly wisdom to administer eve|n to the shadder of propurity formerly l\n",
      "Epoch 55. Time: 12.095, Train loss: 0.980\n",
      "as to its author the all sufficient reas|onableness s too what sense of views and\n",
      "Epoch 56. Time: 12.224, Train loss: 0.978\n",
      "s peculiarly enviable happiness such a p|hysious given in everything else as euro\n",
      "Epoch 57. Time: 12.634, Train loss: 0.977\n",
      " before these opinions as though he had |not communingnated all tempt foothe euro\n",
      "Epoch 58. Time: 12.607, Train loss: 0.976\n",
      "uctive of good in the world of men in so| call as content it is depression has no\n",
      "Epoch 59. Time: 12.022, Train loss: 0.974\n",
      "with artistic curiosity with regard to m|e should have been gequious kind of voin\n",
      "Epoch 60. Time: 11.666, Train loss: 0.972\n",
      "ulgent and preservative care inasmuch as| if it recomes up smilophed man temperar\n",
      "Epoch 61. Time: 12.319, Train loss: 0.969\n",
      "amine more closely what is the scientifi|c man in the instinct are reward a sharo\n",
      "Epoch 62. Time: 12.565, Train loss: 0.969\n",
      " esse be the deed of a free will the bas|is within us is forgemes the attaptition\n",
      "Epoch 63. Time: 12.548, Train loss: 0.967\n",
      "in its widest sense perhaps not be the e|ping in the same true find is arcatefula\n",
      "Epoch 64. Time: 12.055, Train loss: 0.966\n",
      " has been produced a gregarious animal s|ick ourponation of purponsibarian antiqu\n",
      "Epoch 65. Time: 11.813, Train loss: 0.965\n",
      "is this fear of the man in the german sp|irituality superying incorponal observei\n",
      "Epoch 66. Time: 11.380, Train loss: 0.962\n",
      "s within it refrain from doing to each o|r insists in those is men light i wasces\n",
      "Epoch 67. Time: 11.383, Train loss: 0.966\n",
      "w happiness including especially the joy|igis something the present fixts and eve\n",
      "Epoch 68. Time: 12.047, Train loss: 0.959\n",
      "in the general course of conduct indeed |beyond to freedom than god enyourn and d\n",
      "Epoch 69. Time: 12.580, Train loss: 0.959\n",
      "esty and reciprocal usefulness what does| not spirit the philosopher not it and a\n",
      "Epoch 70. Time: 12.488, Train loss: 0.958\n",
      "istian in the assertions of the founder |to natural supposing to the engles natur\n",
      "Epoch 71. Time: 12.738, Train loss: 0.958\n",
      " purposes of men demand their continuanc|e and the opposite on gener generally in\n",
      "Epoch 72. Time: 12.585, Train loss: 0.957\n",
      "n this manner and to imitate all the stu|ble people ethics sade them in deflient \n",
      "Epoch 73. Time: 12.368, Train loss: 0.956\n",
      "christianity as antiquity when on a sund|amental exists after enlighten them forb\n",
      "Epoch 74. Time: 12.148, Train loss: 0.954\n",
      "enial of this or that a prohibition to a| great thinking the great friendship he \n",
      "Epoch 75. Time: 11.951, Train loss: 0.953\n",
      " of the same kind ye coming ones ye new |possible pewelly a humanity as such this\n",
      "Epoch 76. Time: 11.390, Train loss: 0.955\n",
      "of jesuitism which has always understood| hardly really elevation and fundamental\n",
      "Epoch 77. Time: 11.427, Train loss: 0.951\n",
      "n circumstances i love mankind and refer|rement only volundisate and excipard als\n",
      "Epoch 78. Time: 11.312, Train loss: 0.949\n",
      "rcles within the family life blooms and |not free of consoment wich then stand of\n",
      "Epoch 79. Time: 11.521, Train loss: 0.949\n",
      "oling owing to the immense variety of pr|esent a desirable as if any certain fact\n",
      "Epoch 80. Time: 11.799, Train loss: 0.947\n",
      "his depression is something that can be |imprige and worsth and moderating and sa\n",
      "Epoch 81. Time: 11.778, Train loss: 0.948\n",
      "than is the stalk one cannot learn best |often in which sweeten in this art of al\n",
      "Epoch 82. Time: 11.380, Train loss: 0.946\n",
      "d il est bon qu il veut que la virtu cor|rectly selfces is difficult and capacy f\n",
      "Epoch 83. Time: 11.604, Train loss: 0.945\n",
      "terpreting of the world in the manner of| regarding to the who have not the name \n",
      "Epoch 84. Time: 11.655, Train loss: 0.944\n",
      "that because of it the reviving sciences| its self conscience in the store for hi\n",
      "Epoch 85. Time: 11.521, Train loss: 0.944\n",
      "on of truth of eternal popular sensualis| of life that predistions of self evally\n",
      "Epoch 86. Time: 11.339, Train loss: 0.944\n",
      "ad or with divine beings overcomes him t|hat the contrifined of human delighteren\n",
      "Epoch 87. Time: 11.331, Train loss: 0.942\n",
      " taking place a selection of the forms a|nd scon it is almost by taking critain l\n",
      "Epoch 88. Time: 11.388, Train loss: 0.941\n",
      " century when such pictures would not do| whose valuations of philosophy as they \n",
      "Epoch 89. Time: 12.112, Train loss: 0.941\n",
      " what we do best to pass precisely for w|hich the world and did which i gives her\n",
      "Epoch 90. Time: 12.076, Train loss: 0.940\n",
      "hey are happy they have a mode of seizin|g a poisons very paint died the sensatio\n",
      "Epoch 91. Time: 12.148, Train loss: 0.940\n",
      " it is possible to act disinterestedly t|o me when he well as a quannical sufferi\n",
      "Epoch 92. Time: 11.607, Train loss: 0.937\n",
      " to make this attempt it is commanded by| asser soo kand almost thereby shallow i\n",
      "Epoch 93. Time: 11.515, Train loss: 0.938\n",
      "ch the excitation of our passions to whi|ch speak formerly richars words in the a\n",
      "Epoch 94. Time: 11.379, Train loss: 0.939\n",
      " is essentially the morality of utility |harts this is those overach english in t\n",
      "Epoch 95. Time: 11.664, Train loss: 0.937\n",
      "from a weight becoming unendurable is th|e future of institely against stings hav\n",
      "Epoch 96. Time: 12.320, Train loss: 0.937\n",
      "ght time for as it has been disclosed to| the continuance of recognize in the hit\n",
      "Epoch 97. Time: 12.224, Train loss: 0.936\n",
      "r the refined and to sum up shortly ever|y above all this is judget to henoted th\n",
      "Epoch 98. Time: 11.964, Train loss: 0.939\n",
      "f sanctity because of his experience and| night the future will above all in the \n",
      "Epoch 99. Time: 11.984, Train loss: 0.935\n",
      "nemies of religious customs should certa|in rearled when sublimate the its out si\n"
     ]
    }
   ],
   "source": [
    "for ep in range(100):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    model.train()\n",
    "    for X_b, y_b in data:\n",
    "        X_b, y_b = X_b, y_b\n",
    "        optimizer.zero_grad()\n",
    "        answers = model(X_b)\n",
    "        loss = criterion(answers, y_b)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
    "    model.eval()\n",
    "    generate_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b9797-3ae7-4621-ad48-59805d19da3f",
   "metadata": {},
   "source": [
    " # Формирование предсказания y  по послежовательности x\n",
    "\n",
    " Сначала генерируем случайные последовательности по x и по ним сформируем по определенному алгоритцу значение у И попробуем построить модель  \n",
    "\n",
    " Длина последовательности -5\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a55546b-2e11-40af-b34a-46cee1a6e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_TRAIN = 1000\n",
    "COUNT_TEST = 100\n",
    "ARRAY_SIZE = 5\n",
    "def generation_data(len_data:int):\n",
    "    cifre = list(range(0, 9))\n",
    "    X, y = [], []\n",
    "    for item in range(len_data):\n",
    "        x_item = random.sample(list(range(0, 9)), ARRAY_SIZE)\n",
    "\n",
    "        X.append(x_item)\n",
    "        y_add = []\n",
    "        for i,  x_ in enumerate(x_item):\n",
    "            if i == 0:\n",
    "                y_add.append(x_)\n",
    "                continue\n",
    "\n",
    "            y_ = x_ + x_item[0]\n",
    "\n",
    "            if y_ >= 10:\n",
    "                y_ -= 10\n",
    "            y_add.append(y_)\n",
    "        y.append(y_add)\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "X_train, y_train = generation_data(COUNT_TRAIN)\n",
    "X_test, y_test = generation_data(COUNT_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bb920be-e031-4dc1-ba0c-ca6908f82b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 2, 6, 8, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b762f6d-ab5a-42a9-aeb1-4a1447f5467a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 6, 0, 2, 5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ce2e4a5-9168-4163-9485-6314a44968bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dictionary_size, embedding_size, num_hiddens):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(dictionary_size, embedding_size)\n",
    "        self.hidden = nn.LSTM(embedding_size, num_hiddens, batch_first=True)\n",
    "        self.output = nn.Linear(num_hiddens, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "\n",
    "        x, _ = self.hidden(out)\n",
    "        predictions = self.output(x)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9edcc6e9-6a73-4090-939c-040b1944ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)\n",
    "\n",
    "model = NeuralNetwork(10, 30, 54  )\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da73e867-9b4f-4939-8c48-26a94f8544db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 3.0059, test RMSE 2.9829\n",
      "Epoch 50: train RMSE 0.0624, test RMSE 0.1013\n",
      "Epoch 100: train RMSE 0.0323, test RMSE 0.0552\n",
      "Epoch 150: train RMSE 0.0509, test RMSE 0.0634\n",
      "Epoch 200: train RMSE 0.0243, test RMSE 0.0404\n",
      "Epoch 250: train RMSE 0.0159, test RMSE 0.0379\n",
      "Epoch 300: train RMSE 0.0134, test RMSE 0.0323\n",
      "Epoch 350: train RMSE 0.0171, test RMSE 0.0303\n",
      "Epoch 400: train RMSE 0.0104, test RMSE 0.0278\n",
      "Epoch 450: train RMSE 0.0117, test RMSE 0.0268\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        y_batch = y_batch.flatten().to(dtype=torch.float32)\n",
    "        y_pred = y_pred.squeeze().flatten().to(dtype=torch.float32)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    if epoch % 50 != 0:\n",
    "        continue\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        y_pred = y_pred.squeeze().flatten().to(dtype=torch.float32)\n",
    "        y_train_f = y_train.flatten().to(dtype=torch.float32)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train_f))\n",
    "\n",
    "        y_pred = model(X_test)\n",
    "        y_pred = y_pred.squeeze().flatten().to(dtype=torch.float32)\n",
    "        y_test_f = y_test.flatten().to(dtype=torch.float32)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test_f))\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74e66498-15c1-41b4-bc07-e465a05a03a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 1, 8, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cdc7dd4-5120-4da7-a76e-60f5d281ea93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 1, 8, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73d86acf-f323-4f0a-9aea-a1dfbe4bb69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0, 4.0, 1.0, 8.0, 3.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = model(X_test[10])\n",
    "y_pre = y_pre.squeeze().flatten().tolist()\n",
    "y_pre = [ round(item,0) for item in y_pre]\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385769e9-87bf-4557-9ebc-7f91c90f9405",
   "metadata": {},
   "source": [
    "### Вывод модель строена  - она достаточно точно предсказывает поведение y\n",
    "Попробуем другую модель - без  слоя Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "daba952c-de22-4f97-9175-11ebf3681483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLST(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ccf3945-6cb5-4c3d-b700-f5f4b7b55ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelLST(5, 50, 5  )\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab75c43c-582e-46d0-8007-dea5573fa389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 2.8372, test RMSE 2.8621\n",
      "Epoch 50: train RMSE 1.5798, test RMSE 1.6401\n",
      "Epoch 100: train RMSE 0.7683, test RMSE 0.7794\n",
      "Epoch 150: train RMSE 0.3582, test RMSE 0.3642\n",
      "Epoch 200: train RMSE 0.1879, test RMSE 0.2131\n",
      "Epoch 250: train RMSE 0.1447, test RMSE 0.1517\n",
      "Epoch 300: train RMSE 0.2109, test RMSE 0.2109\n",
      "Epoch 350: train RMSE 0.1004, test RMSE 0.1166\n",
      "Epoch 400: train RMSE 0.1240, test RMSE 0.1303\n",
      "Epoch 450: train RMSE 0.1127, test RMSE 0.1262\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch.to(dtype=torch.float32))\n",
    "\n",
    "        y_batch = y_batch.flatten().to(dtype=torch.float32)\n",
    "        y_pred = y_pred.squeeze().flatten().to(dtype=torch.float32)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    if epoch % 50 != 0:\n",
    "        continue\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        y_pred = y_pred.squeeze().flatten().to(dtype=torch.float32)\n",
    "        y_train_f = y_train.flatten().to(dtype=torch.float32)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train_f))\n",
    "\n",
    "        y_pred = model(X_test)\n",
    "        y_pred = y_pred.squeeze().flatten().to(dtype=torch.float32)\n",
    "        y_test_f = y_test.flatten().to(dtype=torch.float32)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test_f))\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f89db68e-50af-4863-bdae-995f2dc6f6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 1, 8, 2, 7],\n",
       "        [5, 8, 7, 2, 6]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c44df59-1ffb-4253-89d0-c332335e062c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 4, 8, 3, 5, 3, 2, 7, 1]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:2].flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c6ecfe4a-625b-498b-ae3f-4c68da89a0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 3, 7, 3, 4, 3, 2, 6, 0]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = model(X_test[0:2])\n",
    "y_pre = y_pre.squeeze().flatten().tolist()\n",
    "y_pre = [ int(item) for item in y_pre]\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323048d3-3e3f-4a48-b1ce-709192406581",
   "metadata": {},
   "source": [
    "# Выводы \n",
    "- применение LSTM для решения лекционного практического задания\n",
    "- построена модели для предсказания значения y - слой  Embedding нужен ТОчность значительно возрастает! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49479f-283d-4e36-8134-74c3ac164d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
